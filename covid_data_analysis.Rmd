```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale(category = "LC_ALL", locale = "en_US.UTF-8")
```

```{r}
#rm(data,data_non_solidarity,data_solidarity,emDict_raw)
rm(list = ls()) 
#install.packages("slam")
library(slam)
library(textcat)
#library(cldr)
library(entropart)
library(boot)
library(vegan)
library(simboot)
#update.packages()
library(tidyverse)
#library(tokenizers)
library(mgcv)
library(twitteR)
library(plyr)
library(dplyr)
library(ROAuth)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(stringi)
#library(sentiment)
library(SnowballC)
library(tm)
library(RColorBrewer)
```


```{r}
setwd("/Users/vidhushinisrinivasan/Documents/R_Scripts_2/")
folder <- "/Users/vidhushinisrinivasan/Documents/R_Scripts_2/COVID-19/"      # path to folder that holds multiple .csv files
file_list <- list.files(path=folder, pattern="*.csv") # create list of all .csv files in folder

# read in each .csv file in file_list and rbind them into a data frame called data_irma 
data_covid <- 
  do.call("rbind", 
          lapply(file_list, 
                 function(x) 
                 read.csv(paste(folder, x, sep=''), 
                 stringsAsFactors = FALSE, fileEncoding = "UTF-8")))
```


```{r}
rm(file_list,folder)
```


```{r}
names(data_covid)
```




```{r}
nrow(data_covid)
```

```{r}
unique(data_covid$lang)
```



```{r}
lang_groups= data_covid%>%
  group_by(user_lang=data_covid$lang) %>%
  dplyr::summarize(n=n())
print(sum(lang_groups$n))
```



```{r}
library(ggplot2)
library(scales)
ggplot(lang_groups, aes(x = reorder(user_lang, -n), y = n)) +
         geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle = -90, hjust = 0)) +
        scale_y_continuous(labels = comma)
```
```{r}
lang_groups= data_covid%>%
  group_by(user_lang=data_covid$lang) 
 
```

```{r}
lang_groups %>%
  dplyr::summarize(n=n())%>%arrange(-n)
```


We can see that English, Spanish and French are the top three languages in paris dataset. Let's analyse the entropy of these three languages

```{r}
data_covid_filtered = data_covid %>%dplyr::filter(!str_detect(text,"(RT|via)((?:\\b\\W*@\\w+)+)"))
```

##Removing retweets

```{r}
lang_groups= data_covid_filtered%>%
  group_by(user_lang=data_covid_filtered$lang) %>%
  dplyr::summarize(n=n()) %>% arrange(-n)
print(sum(lang_groups$n))
```

```{r}
write.csv(data_covid_filtered,"covid-original.csv")
write.csv(data_covid,"covid-full.csv")
```

```{r}
lang_groups 
```



```{r}
library(ggplot2)
library(scales)
ggplot(lang_groups, aes(x = reorder(user_lang, -n), y = n)) +
         geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle = -90, hjust = 0)) +
        scale_y_continuous(labels = comma)
```

#Load the emoji dictionary containing emoji's and the unicode values
```{r}
emDict_raw <- read.csv2("emDict.csv") %>% 
      select(EN, utf8, unicode) %>% 
      dplyr::rename(description = EN, r_encoding = utf8)
```

```{r}
library(Unicode)
```
#pre-process skin ones if necessary
```{r}
# plain skin tones
skin_tones <- c("light skin tone", 
                "medium-light skin tone", 
                "medium skin tone",
                "medium-dark skin tone", 
                "dark skin tone")
# remove plain skin tones and remove skin tone info in description
emDict <- emDict_raw %>%
  # remove plain skin tones emojis
  filter(!description %in% skin_tones) %>%
  # remove emojis with skin tones info, e.g. remove woman: light skin tone and only
  # keep woman
 filter(!grepl(":", description)) %>%
 mutate(description = tolower(description)) %>%
 mutate(unicode = as.u_char(unicode))
```


```{r}
count_matches <- function(string, matchto, description) {
  
  vec <- str_count(string, matchto)
  matches <- which(vec != 0)
  
  descr <- NA
  cnt <- NA
  
  if (length(matches) != 0) {
    
    descr <- description[matches]
    cnt <- vec[matches]
    
  } 
  
  df <- data.frame(text = string, description = descr, count = cnt)
  
  
  return(df)
  
}
```




```{r}
require("parallel")
parallel_match<- function(texts, matchto, description,mc.cores = 4) {
emojis_matching <- function(txt,matchto, description) {
   txt %>% lapply(count_matches, matchto = matchto, description = description) %>%
    bind_rows
}
mclapply(X = texts,FUN = emojis_matching, matchto, description, mc.cores = mc.cores) %>%
   bind_rows

}
```


```{r}
# tweets cleaning pipe
cleanPosts <- function(text) {
  clean_texts <- text %>%
    gsub("<.*>", "", .) %>% # remove emojis
    gsub("&amp;", "", .) %>% # remove &
    gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", .) %>% # remove retweet entities
    gsub("@\\w+", "", .) %>% # remove at people
    hashgrep %>%
    gsub("[[:punct:]]", "", .) %>% # remove punctuation
    gsub("[[:digit:]]", "", .) %>% # remove digits
    gsub("http\\w+", "", .) %>% # remove html links
    iconv(from = "latin1", to = "ASCII", sub="") %>% # remove emoji and bizarre signs
    gsub("[ \t]{2,}", " ", .) %>% # remove unnecessary spaces
    gsub("^\\s+|\\s+$", "", .) %>% # remove unnecessary spaces
    tolower
  return(clean_texts)
}
```

```{r}
# function that separates capital letters hashtags
hashgrep <- function(text) {
  hg <- function(text) {
    result <- ""
    while(text != result) {
      result <- text
      text <- gsub("#[[:alpha:]]+\\K([[:upper:]]+)", " \\1", text, perl = TRUE)
    }
    return(text)
  }
  unname(sapply(text, hg))
}

```


```{r}
# reference website
url <- "http://kt.ijs.si/data/Emoji_sentiment_ranking/index.html"
```

```{r}
library(Unicode)
```



```{r}
library("rvest")
emojis_raw <- url %>%
  read_html() %>%
  html_table() %>%
  data.frame %>%
  dplyr::select(-Image.twemoji., -Sentiment.bar.c.i..95..)
names(emojis_raw) <- c("char", "unicode", "occurrences", "position", "negative", "neutral", "positive", "sentiment_score", "description", "block")
```

```{r}
emojis_raw[emojis_raw$description=="red heart",]
```

```{r}

# change numeric unicode to character unicode to be able to match with emDict 
emojis <- emojis_raw %>%
  dplyr::mutate(unicode = as.u_char(unicode)) %>%
  dplyr::mutate(description = tolower(description)) 

#str(emojis)
```


```{r}
# merge with emDict to get encoding
emojis_merged <- emojis %>%
  merge(emDict, by = "unicode")
```


```{r}
head(emojis_merged)
```



```{r}
new_matchto <- emojis_merged$r_encoding
new_description <- emojis_merged$description.y
sentiment <- emojis_merged$sentiment_score
neutral <- emojis_merged$neutral
```


```{r}
new_data_covid <- data_covid_filtered %>% 
  mutate(text = iconv(text, from = "latin1", to = "ascii", sub = "byte"))
```

```{r}
data_covid_filtered <- read.csv("covid-original.csv")
```

```{r}
data_covid_filtered
```


```{r}
data_covid_filtered_en <- data_covid_filtered %>% dplyr::filter(lang=="en")
```


```{r}
new_data_covid_en <- data_covid_filtered_en %>% 
  mutate(text = iconv(text, from = "latin1", to = "ascii", sub = "byte"))
```

```{r}
head(data_covid_filtered_en)
```


#get the rank of emoji's 
```{r}
library(dplyr)
require(parallel) 
raw_tweets <- parallel_match(new_data_covid_en$text, new_matchto, new_description, mc.cores=4)%>% 
  filter(text != "") %>% filter(!is.na(count)) 
  
```


```{r}
write.csv(raw_tweets, "raw_tweets_full_covid.csv")
```

```{r}
raw_tweets <- read.csv("raw_tweets_english_covid.csv")
```

```{r}
head(raw_tweets, 5)
```

```{r}
unique(raw_tweets$description)
```

```{r}
raw_tweets_altered <- cbind(raw_tweets,unique_count=1)
```

```{r}
emoji_categories = read.csv2("emoji_covid.csv",header = TRUE, sep = ",")
```


```{r}
head(emoji_categories, 5)
```

```{r}
raw_tweets_altered <- cbind(raw_tweets,unique_count=1)
```


### Emoji Diversity
```{r}
library(dplyr)
Face_emoji<- emoji_categories%>%group_by(description)%>%dplyr::filter(Category=="Face")%>%dplyr::summarise(count=sum(count))%>%arrange(-count)
Object_emoji<- emoji_categories%>%group_by(description)%>%dplyr::filter(Category=="Object")%>%dplyr::summarise(count=sum(count))%>%arrange(-count)
Gesture_emoji<- emoji_categories%>%group_by(description)%>%dplyr::filter(Category=="Gesture")%>%dplyr::summarise(count=sum(count))%>%arrange(-count)
```

```{r}
Face_emoji
```

```{r}
Face_emoji_text<- emoji_all_tweets_categories%>%dplyr::filter(Category=="Face")
Object_emoji_text<- emoji_all_tweets_categories%>%dplyr::filter(Category=="Object")
Gesture_emoji_text<- emoji_all_tweets_categories%>%dplyr::filter(Category=="Gesture")
```

```{r}
nrow(Face_emoji)
nrow(Object_emoji)
nrow(Gesture_emoji)
```
```{r}
Gesture_emoji
```
```{r}
raw_tweets_altered
```


```{r}
# merge with emDict to get encoding
#emoji_all_tweets_categories <- raw_tweets_altered %>%
#  merge(emoji_categories, by = "description")

emoji_all_tweets_categories <- raw_tweets_altered %>%
  merge(emoji_categories, by = "description")
```


```{r}
emoji_all_tweets_categories
```



```{r}
tweets_classified<-emoji_all_tweets_categories %>% group_by(text,Category) %>% 
       dplyr::summarise(description = paste(description, collapse =", ") ,total_count =sum(count.x), Total_unique_count = sum(unique_count)) %>% 
       spread(Category, total_count) %>% 
       dplyr::ungroup() %>%
       dplyr::transmute(text=text,description=description, face_count = Face , object_count = Object, gesture_count = Gesture, total_unique_count = Total_unique_count)
```

```{r}
tweets_classified[is.na(tweets_classified)] <- 0
```

```{r}
face_only <- dplyr::filter(tweets_classified, (face_count!= 0 & object_count==0 & gesture_count==0))
object_only <- dplyr::filter(tweets_classified, (face_count== 0 & object_count!=0 & gesture_count==0))
gesture_only <-dplyr::filter(tweets_classified, (face_count== 0 & object_count==0 & gesture_count!=0))
face_object_only <- dplyr::filter(tweets_classified, (face_count!= 0 & object_count!=0 & gesture_count==0))
face_gesture_only <- dplyr::filter(tweets_classified, (face_count!= 0 & object_count==0 & gesture_count!=0))
object_gesture_only <- dplyr::filter(tweets_classified, (face_count== 0 & object_count!=0 & gesture_count!=0))
all_three <- dplyr::filter(tweets_classified, (face_count!= 0 & object_count!=0 & gesture_count!=0))

```


```{r}
nrow(face_only)
nrow(object_only)
nrow(gesture_only)
nrow(face_object_only)
nrow(object_gesture_only)
nrow(face_gesture_only)
nrow(all_three)
```


```{r}
library("iNEXT")
face_var<-ChaoShannon(face_only$face_count, datatype = "abundance", conf = 0.95)/log(2)
object_var<-ChaoShannon(object_only$object_count, datatype = "abundance", conf = 0.95)/log(2)
gesture_var<-ChaoShannon(gesture_only$gesture_count, datatype = "abundance", conf = 0.95)/log(2)
```


```{r}
gesture_var
```

##Face Emoji

```{r}
 ##Remove emoji
corpus_en_face_emoji <- iconv(face_only$text, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_face_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_face_emoji) 
corpus_en_face_emoji <- Corpus(VectorSource(corpus_en_face_emoji))
##convert text to lowercase
corpus_en_face_emoji <- tm_map(corpus_en_face_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_face_emoji <- tm_map(corpus_en_face_emoji,removePunctuation)
##remove numbers from text
corpus_en_face_emoji <- tm_map(corpus_en_face_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_face_emoji <- tm_map(corpus_en_face_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_face_emoji <- tm_map(cleaned_corpus_en_face_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_face_emoji <- tm_map(cleaned_corpus_en_face_emoji,stripWhitespace)
###
cleaned_corpus_en_face_emoji <- tm_map(cleaned_corpus_en_face_emoji,removeWords,c('paris'))
cleaned_corpus_en_face_emoji <- tm_map(cleaned_corpus_en_face_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_face_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_face_emoji <- tm_map(cleaned_corpus_en_face_emoji,stemDocument)
cleaned_corpus_en_face_emoji <- tm_map(cleaned_corpus_en_face_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_face_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_face_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```
```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_face_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_face_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_face_emoji)/log(2)
H
```

```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_face_emoji, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```
```{r}
sapply(cleaned_corpus_en_face_emoji,function(x)length(unlist(gregexpr(" ",x)))+1)
```

```{r}
bootstrapped_tweetlength <- sapply(cleaned_corpus_en_face_emoji,function(x)length(unlist(gregexpr(" ",x)))+1)
```

```{r}
bootstrapped_tweetlength<-as.data.frame(bootstrapped_tweetlength)
names(bootstrapped_tweetlength)[1]<-"Tweet_Length"
```


```{r}
face_boot<-cbind(bootstrapped_tweetlength,isEmoji=1)
```

##Object EMoji
```{r}
 ##Remove emoji
corpus_en_non_face_emoji <- iconv(object_only$text, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_non_face_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_non_face_emoji) 
corpus_en_non_face_emoji <- Corpus(VectorSource(corpus_en_non_face_emoji))
##convert text to lowercase
corpus_en_non_face_emoji <- tm_map(corpus_en_non_face_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_non_face_emoji <- tm_map(corpus_en_non_face_emoji,removePunctuation)
##remove numbers from text
corpus_en_non_face_emoji <- tm_map(corpus_en_non_face_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_non_face_emoji <- tm_map(corpus_en_non_face_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_non_face_emoji <- tm_map(cleaned_corpus_en_non_face_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_non_face_emoji <- tm_map(cleaned_corpus_en_non_face_emoji,stripWhitespace)
###
cleaned_corpus_en_non_face_emoji <- tm_map(cleaned_corpus_en_non_face_emoji,removeWords,c('paris'))
cleaned_corpus_en_non_face_emoji <- tm_map(cleaned_corpus_en_non_face_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_non_face_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_non_face_emoji <- tm_map(cleaned_corpus_en_non_face_emoji,stemDocument)
cleaned_corpus_en_non_face_emoji <- tm_map(cleaned_corpus_en_non_face_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_non_face_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_non_face_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_non_face_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_non_face <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_non_face)/log(2)
H
```


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_non_face, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

```{r}
bootstrapped_tweetlength <- sapply(cleaned_corpus_en_non_face_emoji,function(x)length(unlist(gregexpr(" ",x)))+1)
```

```{r}
bootstrapped_tweetlength
```


```{r}
bootstrapped_tweetlength<-as.data.frame(bootstrapped_tweetlength)
names(bootstrapped_tweetlength)[1]<-"Tweet_Length"
```

```{r}
non_face_boot<-cbind(bootstrapped_tweetlength,isEmoji=2)
````

#Gesture Emoji

```{r}
 ##Remove emoji
corpus_en_gesture_emoji <- iconv(gesture_only$text, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_gesture_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_gesture_emoji) 
corpus_en_gesture_emoji <- Corpus(VectorSource(corpus_en_gesture_emoji))
##convert text to lowercase
corpus_en_gesture_emoji <- tm_map(corpus_en_gesture_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_gesture_emoji <- tm_map(corpus_en_gesture_emoji,removePunctuation)
##remove numbers from text
corpus_en_gesture_emoji <- tm_map(corpus_en_gesture_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_gesture_emoji <- tm_map(corpus_en_gesture_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_gesture_emoji <- tm_map(cleaned_corpus_en_gesture_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_gesture_emoji <- tm_map(cleaned_corpus_en_gesture_emoji,stripWhitespace)
###
cleaned_corpus_en_gesture_emoji <- tm_map(cleaned_corpus_en_gesture_emoji,removeWords,c('paris'))
cleaned_corpus_en_gesture_emoji <- tm_map(cleaned_corpus_en_gesture_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_gesture_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_gesture_emoji <- tm_map(cleaned_corpus_en_gesture_emoji,stemDocument)
cleaned_corpus_en_gesture_emoji <- tm_map(cleaned_corpus_en_gesture_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_gesture_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_gesture_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_gesture_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_gesture_face <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_gesture_face)/log(2)
H
```


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_gesture_face, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

```{r}
bootstrapped_tweetlength <- sapply(cleaned_corpus_en_gesture_emoji,function(x)length(unlist(gregexpr(" ",x)))+1)
```

```{r}
bootstrapped_tweetlength
```


```{r}
bootstrapped_tweetlength<-as.data.frame(bootstrapped_tweetlength)
names(bootstrapped_tweetlength)[1]<-"Tweet_Length"
```

```{r}
gesture_boot<-cbind(bootstrapped_tweetlength,isEmoji=3)
```

#All Emoji Together

```{r}
corpus_en <- data_covid_filtered_en$text
corpus_en_emoji <- raw_tweets$text
corpus_en_NO_emoji_text <- subset(corpus_en, !(corpus_en %in% c(corpus_en_emoji)))
corpus_en_NO_emoji_text <- as.data.frame(unique(corpus_en_NO_emoji_text))
names(corpus_en_NO_emoji_text)[1]<-"tweets"
```

##With Emojis Combined

```{r}
 ##Remove emoji
corpus_en_emoji <- iconv(corpus_en_emoji, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_emoji) 
corpus_en_emoji <- Corpus(VectorSource(corpus_en_emoji))
##convert text to lowercase
corpus_en_emoji <- tm_map(corpus_en_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_emoji <- tm_map(corpus_en_emoji,removePunctuation)
##remove numbers from text
corpus_en_emoji <- tm_map(corpus_en_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_emoji <- tm_map(corpus_en_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)
###
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,removeWords,c('paris'))
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stemDocument)
cleaned_corpus_en_emoji <- tm_map(cleaned_corpus_en_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```


```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_emoji)/log(2)
H
```

```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_emoji, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

## NO EMoji

```{r}
 ##Remove emoji
corpus_en_NO_emoji <- iconv(corpus_en_NO_emoji_text$tweets, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_NO_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_NO_emoji) 
corpus_en_NO_emoji <- Corpus(VectorSource(corpus_en_NO_emoji))
##convert text to lowercase
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,removePunctuation)
##remove numbers from text
corpus_en_NO_emoji <- tm_map(corpus_en_NO_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_No_emoji <- tm_map(corpus_en_NO_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_No_emoji <- tm_map(cleaned_corpus_en_No_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_No_emoji <- tm_map(cleaned_corpus_en_No_emoji,stripWhitespace)
###
cleaned_corpus_en_No_emoji <- tm_map(cleaned_corpus_en_No_emoji,removeWords,c('paris'))
cleaned_corpus_en_No_emoji <- tm_map(cleaned_corpus_en_No_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_No_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_No_emoji <- tm_map(cleaned_corpus_en_No_emoji,stemDocument)
cleaned_corpus_en_No_emoji <- tm_map(cleaned_corpus_en_No_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_No_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_No_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_No_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_no_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_no_emoji)/log(2)
H
```


```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_no_emoji, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

```{r}
bootstrapped_tweetlength <- sapply(cleaned_corpus_en_No_emoji,function(x)length(unlist(gregexpr(" ",x)))+1)
```

```{r}
bootstrapped_tweetlength
```


```{r}
bootstrapped_tweetlength<-as.data.frame(bootstrapped_tweetlength)
names(bootstrapped_tweetlength)[1]<-"Tweet_Length"
```

```{r}
no_emoji_boot<-cbind(bootstrapped_tweetlength,isEmoji=0)
```


##I/WE/Neither


```{r}
I_full <-  new_data_covid_en%>%dplyr::filter(str_detect(text,regex('I\\s+ |Me\\s+|Mine\\s+|My\\s+|i\\s+|me\\s+|mine\\s+|my\\s+',ignore_case = T)))
We_full <-  new_data_covid_en%>%dplyr::filter(str_detect(text,regex('We\\s+|Our\\s+|Ours\\s+|Us\\s+|we\\s+|our\\s+|ours\\s+|us\\s+',ignore_case = T)))
Neither_full <-  new_data_covid_en%>%dplyr::filter(!str_detect(text,regex('I\\s+|Me\\s+|Mine\\s+|My\\s+|i\\s+|me\\s+|mine\\s+|my\\s+',ignore_case = T)))%>%dplyr::filter(!str_detect(text,regex('We\\s+|Our\\s+|Ours\\s+|Us\\s+|we\\s+|our\\s+|ours\\s+|us\\s+',ignore_case = T)))
```


## count each pronoun

```{r}
all_words <- data.frame(table(unlist(strsplit(tolower(data_covid_filtered_en$text), " "))))
all_words
```

```{r}
I_all_count = all_words[all_words$Var1=="i", ]
me_all_count = all_words[all_words$Var1=="me", ]
my_all_count = all_words[all_words$Var1=="my", ]
mine_all_count = all_words[all_words$Var1=="mine", ]

We_all_count = all_words[all_words$Var1=="we", ]
Us_all_count = all_words[all_words$Var1=="us", ]
Our_all_count = all_words[all_words$Var1=="our", ]
Ours_all_count = all_words[all_words$Var1=="ours", ]

```

```{r}
all_words[all_words$Var1=="we're", ]
```

```{r}
Ours_all_count
```


```{r}
I_emoji <-  tweets_classified%>%dplyr::filter(str_detect(text,regex('I\\s+ |Me\\s+|Mine\\s+|My\\s+|i\\s+|me\\s+|mine\\s+|my\\s+',ignore_case = T)))
We_emoji <-  tweets_classified%>%dplyr::filter(str_detect(text,regex('We\\s+|Our\\s+|Ours\\s+|Us\\s+|we\\s+|our\\s+|ours\\s+|us\\s+',ignore_case = T)))
Neither_emoji <-  tweets_classified%>%dplyr::filter(!str_detect(text,'I|Me|Mine|My|i|me|mine|my'))%>%dplyr::filter(!str_detect(text,regex('I\\s+|Me\\s+|Mine\\s+|My\\s+|i\\s+|me\\s+|mine\\s+|my\\s+',ignore_case = T)))%>%dplyr::filter(!str_detect(text,regex('We\\s+|Our\\s+|Ours\\s+|Us\\s+|we\\s+|our\\s+|ours\\s+|us\\s+',ignore_case = T)))
```

```{r}
I_emoji
```
```{r}
#install.packages("splitstackshape")
library(splitstackshape) 
```
```{r}
I_emoji_select <- I_emoji%>%select(c("text","description"))
I_emoji_select
We_emoji_select <- We_emoji%>%select(c("text","description"))
Neither_emoji_select <- Neither_emoji%>%select(c("text","description"))
```

```{r}
write.csv(I_emoji_select, "I_tweets_emoji.csv")
write.csv(We_emoji_select, "We_tweets_emoji.csv")
write.csv(Neither_emoji_select, "Neither_tweets_emoji.csv")
```

```{r}
Neither_emoji_select
```


```{r}
Face_emoji_count<- Neither_emoji%>%dplyr::filter(object_count!=0)
count(unique(cSplit(Face_emoji_count,"description",",")$	
description_01))
```

```{r}
Face_emoji_count$description
```

## Get Tweet count
```{r}
tweets_classified$face_count[tweets_classified$face_count > 0] <- 1 
tweets_classified$object_count[tweets_classified$object_count > 0] <- 1 
tweets_classified$gesture_count[tweets_classified$gesture_count > 0] <- 1 
```

```{r}
tweets_classified
```
##Get Unique Count
```{r}
I_emoji_face = subset(I_emoji,(face_count!=0 & object_count==0 & gesture_count==0))
I_emoji_object = subset(I_emoji,(face_count==0 & object_count!=0 & gesture_count==0))
I_emoji_gesture = subset(I_emoji,(face_count==0 & object_count==0 & gesture_count!=0))
We_emoji_face = subset(We_emoji,(face_count!=0 & object_count==0 & gesture_count==0))
We_emoji_object = subset(We_emoji,(face_count==0 & object_count!=0 & gesture_count==0))
We_emoji_gesture = subset(We_emoji,(face_count==0 & object_count==0 & gesture_count!=0))
Neither_emoji_face = subset(Neither_emoji,(face_count!=0 & object_count==0 & gesture_count==0))
Neither_emoji_object = subset(Neither_emoji,(face_count==0 & object_count!=0 & gesture_count==0))
Neither_emoji_gesture = subset(Neither_emoji,(face_count==0 & object_count==0 & gesture_count!=0))
```

#LD for each pronoun-emoji
```{r}
sum(I_emoji_face$total_unique_count)
sum(I_emoji_object$total_unique_count)
sum(I_emoji_gesture$total_unique_count)
sum(We_emoji_face$total_unique_count)
sum(We_emoji_object$total_unique_count)
sum(We_emoji_gesture$total_unique_count)
sum(Neither_emoji_face$total_unique_count)
sum(Neither_emoji_object$total_unique_count)
sum(Neither_emoji_gesture$total_unique_count)

```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_I_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_I_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_I_emoji)/log(2)
H
```

```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_I_emoji, datatype = "abundance", conf = 0.95)/log(2)
```
##I tweets

```{r}
 ##Remove emoji
corpus_en_I_emoji <- iconv(I_emoji_gesture$text, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_I_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_I_emoji) 
corpus_en_I_emoji <- Corpus(VectorSource(corpus_en_I_emoji))
##convert text to lowercase
corpus_en_I_emoji <- tm_map(corpus_en_I_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_I_emoji <- tm_map(corpus_en_I_emoji,removePunctuation)
##remove numbers from text
corpus_en_I_emoji <- tm_map(corpus_en_I_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_I_emoji <- tm_map(corpus_en_I_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_I_emoji <- tm_map(cleaned_corpus_en_I_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_I_emoji <- tm_map(cleaned_corpus_en_I_emoji,stripWhitespace)
###
cleaned_corpus_en_I_emoji <- tm_map(cleaned_corpus_en_I_emoji,removeWords,c('paris'))
cleaned_corpus_en_I_emoji <- tm_map(cleaned_corpus_en_I_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_I_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_I_emoji <- tm_map(cleaned_corpus_en_I_emoji,stemDocument)
cleaned_corpus_en_I_emoji <- tm_map(cleaned_corpus_en_I_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_I_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_I_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_I_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_I_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_I_emoji)/log(2)
H
```

```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_I_emoji, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

##We Tweets

```{r}
 ##Remove emoji
corpus_en_We_emoji <- iconv(We_emoji_gesture$text, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_We_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_We_emoji) 
corpus_en_We_emoji <- Corpus(VectorSource(corpus_en_We_emoji))
##convert text to lowercase
corpus_en_We_emoji <- tm_map(corpus_en_We_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_We_emoji <- tm_map(corpus_en_We_emoji,removePunctuation)
##remove numbers from text
corpus_en_We_emoji <- tm_map(corpus_en_We_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
cleaned_corpus_en_We_emoji <- tm_map(corpus_en_We_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_We_emoji <- tm_map(cleaned_corpus_en_We_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_We_emoji <- tm_map(cleaned_corpus_en_We_emoji,stripWhitespace)
###
cleaned_corpus_en_We_emoji <- tm_map(cleaned_corpus_en_We_emoji,removeWords,c('paris'))
cleaned_corpus_en_We_emoji <- tm_map(cleaned_corpus_en_We_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_We_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_We_emoji <- tm_map(cleaned_corpus_en_We_emoji,stemDocument)
cleaned_corpus_en_We_emoji <- tm_map(cleaned_corpus_en_We_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_We_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_We_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_We_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_We_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_We_emoji)/log(2)
H
```

```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_We_emoji, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

##Neither Tweets


```{r}
 ##Remove emoji
corpus_en_Neither_emoji <- iconv(Neither_emoji_gesture$text, "latin1", "ASCII", sub="")
### remove retweet entities
corpus_en_Neither_emoji <-  gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", corpus_en_Neither_emoji) 
corpus_en_Neither_emoji <- Corpus(VectorSource(corpus_en_Neither_emoji))
##convert text to lowercase
corpus_en_Neither_emoji <- tm_map(corpus_en_Neither_emoji,tolower)
#inspect(corpus[1:5])
##remove punctuations from text
corpus_en_Neither_emoji <- tm_map(corpus_en_Neither_emoji,removePunctuation)
##remove numbers from text
corpus_en_Neither_emoji <- tm_map(corpus_en_Neither_emoji,removeNumbers)
#inspect(corpus[1:5])
##remove stopwords from french language 
##corpus_USA_en_emoji <- tm_map(corpus_USA_en_emoji,removeWords,stopwords("en"))
##remove url from text
removeURL <- function(x) gsub("http[[:alnum:]]*",'',x)
corpus_en_Neither_emoji <- tm_map(corpus_en_Neither_emoji,content_transformer(removeURL))
#inspect(cleaned_corpus[1:5])
##remove emoticons
removeEmoticons <- function(x) gsub("(?::|;|=)(?:-)?(?:\\)|\\(|D|P)",'',x)
cleaned_corpus_en_Neither_emoji <- tm_map(corpus_en_Neither_emoji,content_transformer(removeEmoticons))
   
##remove white space
cleaned_corpus_en_Neither_emoji <- tm_map(cleaned_corpus_en_Neither_emoji,stripWhitespace)
###
cleaned_corpus_en_Neither_emoji <- tm_map(cleaned_corpus_en_Neither_emoji,removeWords,c('paris'))
cleaned_corpus_en_Neither_emoji <- tm_map(cleaned_corpus_en_Neither_emoji,stripWhitespace)
##average word length before stemming
# split words
words_list = strsplit(cleaned_corpus_en_Neither_emoji$content," ")
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
avg_word_len = mean(wsize_per_tweet,na.rm=TRUE)
avg_word_len
##Stemming
cleaned_corpus_en_Neither_emoji <- tm_map(cleaned_corpus_en_Neither_emoji,stemDocument)
cleaned_corpus_en_Neither_emoji <- tm_map(cleaned_corpus_en_Neither_emoji,stripWhitespace)
##avg tweet length
avg_tweet_len <- mean(sapply(cleaned_corpus_en_Neither_emoji,function(x)length(unlist(gregexpr(" ",x)))+1))
avg_tweet_len
length(cleaned_corpus_en_Neither_emoji)
#inspect(cleaned_corpus_en_emoji[1:25])
```

```{r}
tdm <- TermDocumentMatrix(cleaned_corpus_en_Neither_emoji)
#tdm <- as.matrix(tdm)
#print(tdm[1:10,1:20])
#tdm
#tdm <- as.matrix(tdm)
freqSum_en_Neither_emoji <- slam::row_sums(tdm, na.rm = T)
#apply(tdm, 1, sum)
#freqSum_USA_en <- rowSums(tdm)
#freqSum_France_fr 
H=Shannon(freqSum_en_Neither_emoji)/log(2)
H
```

```{r}
library("iNEXT")
var<-ChaoShannon(freqSum_en_Neither_emoji, datatype = "abundance", conf = 0.95)/log(2)
```

```{r}
var
```

##Word cloud

## I
```{r}
library(tm)
#Create a vector containing only the text
text <- I_full$text
text <- cleanPosts(I_full$text)
# Create a corpus  
docs <- Corpus(VectorSource(text))
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 
```


```{r}
dtm.new<-removeSparseTerms(dtm, sparse=0.99)
```

```{r}

matrix <- as.matrix(dtm.new) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```



```{r}
library(wordcloud)
library(RColorBrewer)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```


## WE
```{r}
library(tm)
#Create a vector containing only the text
text <- We_full$text
text <- cleanPosts(We_full$text)
# Create a corpus  
docs <- Corpus(VectorSource(text))
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 

```

```{r}
dtm.new<-removeSparseTerms(dtm, sparse=0.99)
```

```{r}

matrix <- as.matrix(dtm.new) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```

```{r}
library(wordcloud)
library(RColorBrewer)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

## Neither
```{r}
library(tm)
#Create a vector containing only the text
text <- Neither_full$text
text <- cleanPosts(Neither_full$text)
# Create a corpus  
docs <- Corpus(VectorSource(text))
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 


```

```{r}
dtm.new<-removeSparseTerms(dtm, sparse=0.99)
```

```{r}

matrix <- as.matrix(dtm.new) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```

```{r}
library(wordcloud)
library(RColorBrewer)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```


##Face/Object/Gesture
```{r}
library(tm)
#Create a vector containing only the text
text <- Gesture_emoji_text$text
text <- cleanPosts(Gesture_emoji_text$text)
# Create a corpus  
docs <- Corpus(VectorSource(text))
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stemDocument) %>%
  tm_map(removeWords,c('charlottesville','Charlottesville','cville','charlottesvill','charlottesvilleva')) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 
```

```{r}
dtm.new<-removeSparseTerms(dtm, sparse=0.99)
```

```{r}

matrix <- as.matrix(dtm.new) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```


```{r}
library(wordcloud)
library(RColorBrewer)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, scale=c(3.5,0.5),  min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

##All Emoji
#With Emoji WordCLoud
```{r}
library(tm)
#Create a vector containing only the text
text <- corpus_en_emoji
text <- cleanPosts(corpus_en_emoji)
# Create a corpus  
docs <- Corpus(VectorSource(text))
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
   tm_map(stemDocument) %>%
  tm_map(removeWords,c('charlottesville','Charlottesville','cville','charlottesvilleva','cvill')) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 

```

```{r}
dtm.new<-removeSparseTerms(dtm, sparse=0.99)
```

```{r}

matrix <- as.matrix(dtm.new) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```

```{r}
library(wordcloud)
library(RColorBrewer)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

##No Emoji

```{r}
library(tm)
#Create a vector containing only the text
text <- corpus_en_NO_emoji_text$tweets
text <- cleanPosts(corpus_en_NO_emoji_text$tweets)
# Create a corpus  
docs <- Corpus(VectorSource(text))
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
   tm_map(stemDocument) %>%
  tm_map(removeWords,c('charlottesville','Charlottesville','cville','charlottesvilleva','cvill')) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 

```

```{r}
dtm.new<-removeSparseTerms(dtm, sparse=0.99)
```

```{r}

matrix <- as.matrix(dtm.new) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```

```{r}
library(wordcloud)
library(RColorBrewer)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, scale=c(3.5,0.5),  min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

#ANOVA ON TWEET LENGTH
```{r}
boot_combined<- Reduce(function(x, y) merge(x, y, all=TRUE), list(face_boot, non_face_boot,gesture_boot, no_emoji_boot))
```

```{r}
unique(boot_combined$isEmoji)
```

```{r}
boot_combined
```


```{r}
library(dplyr)
group_by(boot_combined, isEmoji) %>%
  dplyr::summarise(
    count = n(),
    mean = mean(Tweet_Length, na.rm = TRUE),
    sd = sd(Tweet_Length, na.rm = TRUE)
  )
```
### no assumption of equal variances
```{r}
oneway.test(Tweet_Length ~ isEmoji, data = boot_combined)
```


```{r}
boot_combined = boot_combined %>%
  mutate_all(as.character)
```


```{r}
summary(aov(formula =Tweet_Length~isEmoji, data = boot_combined))
```

```{r}
write.csv(boot_combined,"anova_covid.csv")
```

#Cooccurence

#Separate the tweets into solidarity and non-solidarity tweets
```{r}
data_irma_solidarity = subset(data_irma, labels==1)
data_irma_non_solidarity = subset(data_irma, labels==-1)
```



#Number of solidarity and non-solidarity tweets
```{r}
nrow(data_irma_solidarity)
nrow(data_irma_non_solidarity)
```

#get the solidarity tweets from the dataframe 
```{r}
sol_irma_tweets <- data_irma_solidarity %>% select(text) 
sol_irma_tweets <- unique(sol_irma_tweets)
```


#get the non-solidarity tweets from the dataframe 
```{r}
non_sol_irma_tweets <- data_irma_non_solidarity %>% select(text) 
non_sol_irma_tweets <- unique(non_sol_irma_tweets)
```

```{r}
nrow(non_sol_irma_tweets)
```

#Load the emoji dictionary containing emoji's and the unicode values
```{r}
emDict_raw <- read.csv2("emDict.csv",sep=";") %>% 
      select(EN, utf8, unicode) %>% 
      dplyr::rename(description = EN, r.encoding = utf8)
```

#pre-process skin ones if necessary
```{r}
# plain skin tones
skin_tones <- c("light skin tone", 
                "medium-light skin tone", 
                "medium skin tone",
                "medium-dark skin tone", 
                "dark skin tone")
# remove plain skin tones and remove skin tone info in description
emDict <- emDict_raw %>%
  # remove plain skin tones emojis
 filter(!description %in% skin_tones) %>%
  # remove emojis with skin tones info, e.g. remove woman: light skin tone and only
  # keep woman
  filter(!grepl(":", description)) %>%
  mutate(description = tolower(description)) %>%
  mutate(unicode = unicode)
```

```{r}
new_data_covid_en
```
#get the rank of emoji's in solidarity tweets

```{r}
new_rank_solidarity <- raw_tweets%>%
  group_by(description)%>% 
  dplyr::summarise(count = sum(count)) %>%arrange(-count) 
```


#get the rank of emoji's in non-solidarity tweets
```{r}
new_rank_non_solidarity <- data_irma_non_solidarity%>%
  group_by(description)%>% 
  dplyr::summarise(count = sum(count)) %>%arrange(-count) 
```


#Filter out tweets with the description "None"
```{r}
raw_texts_irma_sol <- raw_tweets %>% 
  #mutate(text = cleanPosts(text)) %>%
  filter(text != "") %>% 
  filter(description!="None")
```


##get the rank of top 10 emoji's
```{r}
res_rank<-head(new_rank_solidarity, 10)
res_rank
```

#Merge the emoji dictionary with the rank data frame to obtain unicode values for each emoji's
```{r}
res <- merge(res_rank, emDict, by.x = "description", by.y = "description")
res
```

##add variation selector to the red heart to ensure correct display
```{r}
res$unicode[res$description=="red heart"] <- 'U+2764 U+FE0F'
```

```{r}
conv_unicode = as.character(parse(text=shQuote(gsub("U\\+([A-Z0-9]+)", "\\\\U\\1", res$unicode))))
conv_unicode = gsub("[[:space:]]", "", conv_unicode) 
conv_unicode
```

##Frequency chart showing top ten emoji's
```{r}
library(emojifont)
library(ggplot2)
library(gridSVG)
load.emojifont("OpenSansEmoji.ttf")
list.emojifonts()
quartz()
ggplot(res, aes(reorder(description,-count), sprintf("%0.2f", round(count, digits = 2)), label = c(conv_unicode))) +
  geom_bar(stat = "identity") +
  geom_text(col= 'blue',family = "OpenSansEmoji", size = 6, vjust = -.5) +
  scale_x_discrete(breaks = res$description, labels = c(conv_unicode)) +
  ylab("Frequency") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
    axis.ticks.x=element_blank())
ps = grid.export("emoji_solidarity_irma.svg", addClass=T)
```


##Create data table containing emoji's grouping by each tweet and country
```{r}
#install.packages("qdap")
emoji_irma_sol <- unique(raw_tweets)
library(data.table)
emoj_sol_lis_irma<-as.data.table(emoji_irma_sol)[, toString(description), by = list(text)]
emoj_sol_lis_irma
```



##get the tweets and emoji's for regions affected by irma
```{r}
emoj_sol_lis_irma_affected = emoj_sol_lis_irma[which(country %in% c("United States of America","Antigua and Barbuda","Barbados","Saint Martin","Saint Kitts and Nevis","Saint Barthelemy","British Virgin Islands","Anguilla","Puerto Rico","Haiti","Dominican Republic","Dominica","Cuba","Turks and Caicos Islands"))]
```




#get the frequency of each emoji
```{r}
library(stringr)
emoj_sol_lis_irma$V1%>%
  str_split(", | and ") %>%
  unlist %>%
  table %>%
  data.frame %>%
  arrange(c(-Freq))
```


##construct a weighted edge list
```{r}
e <- emoj_sol_lis_irma$V1 %>%
  str_split(", | and ") %>%
  lapply(function(x) {
    expand.grid(x, x,w = 1 / length(x), stringsAsFactors = FALSE)
  }) %>%
  bind_rows
e <- apply(e[, -3], 1, str_sort) %>%
  t %>%
  data.frame(stringsAsFactors = FALSE) %>%
  mutate(w = e$w)
```



#remove keywords connected to themselves in cooccurence pairs
```{r}
e <- group_by(e, X1, X2) %>%
  dplyr::summarise(w = sum(w)) %>% 
  filter(X1 != X2) %>% arrange(-w) 
```

#construct a weighted network
```{r}
#install.packages("igraph")
#install.packages("tnet")
library(tnet)
library(igraph)
library(intergraph)
library(ggnetwork)
library(network)
library(dplyr)
library(ggplot2)
n <- network(e[, -3], directed = FALSE)
network::set.edge.attribute(n, "weight", e$w)
# weighted degree at alpha = 1
t <- as.edgelist(n, attrname = "weight") %>%
  symmetrise_w %>%
  as.tnet %>%
  degree_w
stopifnot(nrow(t) == network.size(n))
network::set.vertex.attribute(n, "degree_w", t[, "output" ])
```

#remove the nodes with a low weighted degree from consideration
```{r}
l <- n %v% "degree_w"
l <- ifelse(l >= median(l), network.vertex.names(n), NA)
stopifnot(length(l) == network.size(n))
#l <- l[!is.na(l)]
#network::set.vertex.attribute(n, "label", l)
```


```{r}
l <- rbind(l, data.frame(description = l))
```


#merge with the emoji dictionary to get unicode value for each emoji
```{r}
res_sol <- merge(l, emDict, by.x = "description", by.y = "description")
res_sol
```
##add variation selector to the red heart to ensure correct display
```{r}
res_sol$unicode[res_sol$description=="red heart"] <- 'U+2764 U+FE0F'
```

```{r}
res_sol$unicode[1]
gsub("U\\+([A-Z0-9]+)", "\\\\U000\\1", res_sol$unicode[1])
```

```{r}
conv_unicode_paris_sol = as.character(parse(text=shQuote(gsub("U\\+([A-Z0-9]+)", "\\\\U\\1", res_sol$unicode))))

#conv_unicode_paris_sol
#class(conv_unicode_paris_sol)
network::set.vertex.attribute(n, "label", conv_unicode_paris_sol)
 network.vertex.names(n)<-c(conv_unicode_paris_sol)

```

```{r}
conv_unicode_paris_sol
```

#create coocurrence network
```{r}
library(emojifont)
library(ggplot2)
library(gridSVG)
load.emojifont("EmojiOne.ttf")
list.emojifonts()
quartz()
ggnetwork(n,layout = "fruchtermanreingold",weights = "weight",n_iter=1500) %>%
ggplot(aes(x, y, xend = xend, yend = yend)) +
  geom_nodetext(aes(label = label),check_overlap=TRUE) +
  scale_size_continuous(range = c(6, 6)) +
  scale_color_gradient2(low = "grey25", midpoint = 0.75, high = "black") +
  guides(size = FALSE, color = FALSE) + 
  theme_blank()
ggsave("covid_full_cluster.pdf")
ps = grid.export("covid_cluster_full.svg", addClass=T)
```
```{r}
H <- 9.37
2^H
```
